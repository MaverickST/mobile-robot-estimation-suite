# State Estimation Examples

Quick reference for running Bayesian filter examples with the omnidirectional robot.

## üìñ Complete Theory

For mathematical derivations, algorithms, and detailed explanations:
- **[Bayesian Filters Theory](../docs/BAYESIAN_FILTERS.md)** - EKF, UKF, PF complete methodology
- **[Q Matrix Computation](../docs/Q_MATRIX_COMPUTATION.md)** - Process noise determination

---

## üöÄ Quick Start

Run examples using the unified execution script:

```powershell
# From project root
.\run.ps1 ekf        # Extended Kalman Filter
.\run.ps1 ukf        # Unscented Kalman Filter
.\run.ps1 pf         # Particle Filter
.\run.ps1 compare    # Compare all filters
```

---

## üìã Available Scripts

### Main Filters (Executable with `run.ps1`)

| Script | Filter | Purpose | Command |
|--------|--------|---------|---------|
| **`ekf_omnidirectional.py`** | EKF | Extended Kalman Filter with Jacobians | `.\run.ps1 ekf` |
| **`ukf_omnidirectional.py`** | UKF | Unscented Kalman Filter (sigma points) | `.\run.ps1 ukf` |
| **`pf_omnidirectional.py`** | PF | Particle Filter (N=2000, SIR algorithm) | `.\run.ps1 pf` |
| **`compare_filters.py`** | All | Side-by-side comparison of EKF/UKF/PF | `.\run.ps1 compare` |

### Utility Scripts

| Script | Purpose | Command |
|--------|---------|---------|
| **`example_Q_from_identification.py`** | Compute Q matrix from robot model | `.\run.ps1 compute_q` |
| `custom_model.py` | Example with differential drive robot | (manual execution) |
| `example_gp_correction.py` | GP-based model correction (experimental) | (manual execution) |
| `generate_embedded_data.py` | Export data for ESP32 deployment | (manual execution) |
| `trajectory_generators.py` | Trajectory generation utilities | (library module) |

---

## ‚öôÔ∏è Configuration

Edit constants at the beginning of each script:

```python
# Data source
USE_EXPERIMENTAL_DATA = True      # True: real robot data, False: synthetic
EXPERIMENT_NUMBER = 2             # Experiment 1-10

# Simulation parameters
N_POINTS = 1000                   # Number of time steps
DT = 0.01                         # Sampling period (100 Hz)
```

### Data Files

**Experimental data:**
- **Sensors:** `data/sensors/expN.txt` (IMU + encoders)
- **Ground truth:** `data/processed/trajectories/traj_vid_N.csv` (video + IMU fusion)

**Format:**
- Frequency: 100 Hz
- Duration: 10 seconds (1000 samples)
- State: [x, y, œÜ, vx, vy, œâ]
- Controls: [ax_body, ay_body]
- Measurements: [vx_body, vy_body, œâ, œÜ]

---

## üìä Output

Results saved to: `results/estimation/<filter_name>/`

**Generated files:**
- `<filter>_trayectoria_2d.png` - 2D trajectory with orientation arrows
- `<filter>_estados_temporales.png` - Time-series plots of all states
- `<filter>_metricas.csv` - Performance metrics (RMSE, MAE, NEES)

**Metrics:**
- **RMSE:** Root mean square error
- **MAE:** Mean absolute error
- **NEES:** Normalized estimation error squared (filter consistency)
- **NIS:** Normalized innovation squared

---

## üí° Filter-Specific Notes

### Extended Kalman Filter (EKF)
- **Requires:** Jacobians F and H (provided by robot model)
- **Angle handling:** Automatic residual wrapping for œà
- **Best for:** Weakly nonlinear systems, fast execution

### Unscented Kalman Filter (UKF)
- **Sigma points:** 13 points (2n+1 for n=6)
- **Parameters:** Œ±=0.5, Œ≤=2.0, Œ∫=0.0
- **Custom functions:** Circular mean and residual for angles
- **Best for:** Moderate nonlinearities, no Jacobian needed

### Particle Filter (PF)
- **Particles:** N=2000
- **Resampling:** Systematic (every iteration)
- **ESS monitoring:** Effective sample size tracking
- **Best for:** Severe nonlinearities, non-Gaussian noise

---

## üìö Additional Documentation

### Python API
See [State Estimation_Python/README.md](../README.md) for API reference.

### C Implementation
See [State Estimation_C/README.md](../../State%20Estimation_C/README.md) for embedded deployment.

### Robot Model
See [Robot_Identification/README.md](../../Robot_Identification/README.md) for parameter identification.

---

## ‚úÖ Example Execution

```powershell
# 1. Run UKF on experimental data from experiment 2
.\run.ps1 ukf

# Output:
# ========================================================
#   Unscented Kalman Filter
# ========================================================
# Description: UKF for omnidirectional robot state estimation
#
# Checking virtual environment...
# ‚úì Virtual environment found: E:\Project\.venv
# ‚úì Virtual environment already active
#
# Working Directory: State Estimation_Python
# Script:            examples\ukf_omnidirectional.py
# ========================================================
#   Executing...
# ========================================================
# [UKF] Running Unscented Kalman Filter...
# [UKF] Experiment: 2
# [UKF] Time steps: 1000 (10.00 s @ 100 Hz)
# ...
# [Results] RMSE_x: 0.0284 m
# [Results] RMSE_phi: 0.0512 rad
# [Results] NEES: 7.23 (valid range: [1.24, 14.45])
# ‚úì Results saved to: results/estimation/ukf/
# ========================================================
#   ‚úì EXECUTION COMPLETED SUCCESSFULLY
# ========================================================
```

---

**For complete mathematical theory, derivations, and implementation details:**
‚Üí **[See docs/BAYESIAN_FILTERS.md](../docs/BAYESIAN_FILTERS.md)**

---

## Marco Te√≥rico Com√∫n

### Modelo de Estado

**Vector de estado** (marco global):
$$
\mathbf{x} = \begin{bmatrix} x \\ y \\ \psi \\ v_x \\ v_y \\ \omega \end{bmatrix} \in \mathbb{R}^6
$$

Donde:
- $(x, y)$: Posici√≥n en marco global [m]
- $\psi$: Orientaci√≥n (√°ngulo de rumbo) [rad]
- $(v_x, v_y)$: Velocidad en marco global [m/s]
- $\omega$: Velocidad angular [rad/s]

**Vector de control** (aceleraciones en marco del cuerpo):
$$
\mathbf{u} = \begin{bmatrix} a_{x,b} \\ a_{y,b} \end{bmatrix} \in \mathbb{R}^2
$$

**Vector de medici√≥n**:
$$
\mathbf{z} = \begin{bmatrix} v_{x,b} \\ v_{y,b} \\ \omega \\ \psi \end{bmatrix} \in \mathbb{R}^4
$$

Donde $(v_{x,b}, v_{y,b})$ son velocidades en marco del cuerpo derivadas de encoders mediante cinem√°tica inversa.

### Din√°micas del Sistema

El modelo implementado en [`omnidirectional.py`](../state_estimation/models/omnidirectional.py#L90) realiza la transformaci√≥n de aceleraciones del marco del cuerpo al marco global:

$$
\begin{aligned}
\mathbf{R}(\psi) &= \begin{bmatrix} \cos\psi & -\sin\psi \\ \sin\psi & \cos\psi \end{bmatrix} \\[0.5em]
\begin{bmatrix} a_x \\ a_y \end{bmatrix}_{\text{global}} &= \mathbf{R}(\psi) \begin{bmatrix} a_{x,b} \\ a_{y,b} \end{bmatrix}
\end{aligned}
$$

**Ecuaciones discretas de estado** (Euler con $\Delta t = 0.01$ s):

$$
\mathbf{x}_{k+1} = f(\mathbf{x}_k, \mathbf{u}_k) = \begin{bmatrix}
x_k + v_{x,k} \cdot \Delta t \\
y_k + v_{y,k} \cdot \Delta t \\
\psi_k + \omega_k \cdot \Delta t \\
v_{x,k} + a_x \cdot \Delta t \\
v_{y,k} + a_y \cdot \Delta t \\
\omega_k
\end{bmatrix}
$$

Donde $a_x = \cos\psi \cdot a_{x,b} - \sin\psi \cdot a_{y,b}$ y $a_y = \sin\psi \cdot a_{x,b} + \cos\psi \cdot a_{y,b}$.

**Nota:** La velocidad angular $\omega$ sigue un modelo de paseo aleatorio (proceso de Wiener), asumiendo cambios suaves entre pasos temporales.

### Modelo de Medici√≥n

Transformaci√≥n de velocidades globales a marco del cuerpo:

$$
h(\mathbf{x}) = \begin{bmatrix}
\cos\psi \cdot v_x + \sin\psi \cdot v_y \\
-\sin\psi \cdot v_x + \cos\psi \cdot v_y \\
\omega \\
\psi
\end{bmatrix}
$$

Implementado en [`omnidirectional.py`](../state_estimation/models/omnidirectional.py#L152).

---

## 1. Extended Kalman Filter (EKF)

**Archivo de implementaci√≥n:** [`filters/extended.py`](../state_estimation/filters/extended.py)  
**Script de ejecuci√≥n:** [`ekf_omnidirectional.py`](ekf_omnidirectional.py)

### 1.1 Fundamento Matem√°tico

El EKF linealiza localmente el sistema no lineal mediante expansiones de Taylor de primer orden alrededor del estado estimado, utilizando las matrices jacobianas $\mathbf{F}$ y $\mathbf{H}$.

#### Paso de Predicci√≥n

**Estado a priori:**
$$
\hat{\mathbf{x}}_{k|k-1} = f(\hat{\mathbf{x}}_{k-1|k-1}, \mathbf{u}_k)
$$

**Covarianza a priori:**
$$
\mathbf{P}_{k|k-1} = \mathbf{F}_k \mathbf{P}_{k-1|k-1} \mathbf{F}_k^T + \mathbf{Q}_k
$$

#### Paso de Actualizaci√≥n

**Residual (innovaci√≥n):**
$$
\mathbf{y}_k = \mathbf{z}_k - h(\hat{\mathbf{x}}_{k|k-1})
$$

**Covarianza de innovaci√≥n:**
$$
\mathbf{S}_k = \mathbf{H}_k \mathbf{P}_{k|k-1} \mathbf{H}_k^T + \mathbf{R}_k
$$

**Ganancia de Kalman:**
$$
\mathbf{K}_k = \mathbf{P}_{k|k-1} \mathbf{H}_k^T \mathbf{S}_k^{-1}
$$

**Estado a posteriori:**
$$
\hat{\mathbf{x}}_{k|k} = \hat{\mathbf{x}}_{k|k-1} + \mathbf{K}_k \mathbf{y}_k
$$

**Covarianza a posteriori** (forma de Joseph para estabilidad num√©rica):
$$
\mathbf{P}_{k|k} = (\mathbf{I} - \mathbf{K}_k \mathbf{H}_k) \mathbf{P}_{k|k-1} (\mathbf{I} - \mathbf{K}_k \mathbf{H}_k)^T + \mathbf{K}_k \mathbf{R}_k \mathbf{K}_k^T
$$

### 1.2 C√°lculo de Jacobianos

#### Jacobiano de Din√°mica: $\mathbf{F} = \frac{\partial f}{\partial \mathbf{x}}$

Implementado en [`omnidirectional.py`](../state_estimation/models/omnidirectional.py#L193):

$$
\mathbf{F}_k = \begin{bmatrix}
1 & 0 & 0 & \Delta t & 0 & 0 \\
0 & 1 & 0 & 0 & \Delta t & 0 \\
0 & 0 & 1 & 0 & 0 & \Delta t \\
0 & 0 & \frac{\partial a_x}{\partial \psi} \Delta t & 1 & 0 & 0 \\
0 & 0 & \frac{\partial a_y}{\partial \psi} \Delta t & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix}
$$

Donde los t√©rminos de acoplamiento rotacional son:

$$
\begin{aligned}
\frac{\partial a_x}{\partial \psi} &= -\sin\psi \cdot a_{x,b} - \cos\psi \cdot a_{y,b} \\
\frac{\partial a_y}{\partial \psi} &= \cos\psi \cdot a_{x,b} - \sin\psi \cdot a_{y,b}
\end{aligned}
$$

**Interpretaci√≥n f√≠sica:** Estos t√©rminos capturan c√≥mo cambios en la orientaci√≥n afectan la aceleraci√≥n en marco global, representando el acoplamiento cinem√°tico entre rotaci√≥n y traslaci√≥n.

#### Jacobiano de Medici√≥n: $\mathbf{H} = \frac{\partial h}{\partial \mathbf{x}}$

Implementado en [`omnidirectional.py`](../state_estimation/models/omnidirectional.py#L216):

$$
\mathbf{H}_k = \begin{bmatrix}
0 & 0 & -\sin\psi \cdot v_x + \cos\psi \cdot v_y & \cos\psi & \sin\psi & 0 \\
0 & 0 & -\cos\psi \cdot v_x - \sin\psi \cdot v_y & -\sin\psi & \cos\psi & 0 \\
0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & 0
\end{bmatrix}
$$

**Interpretaci√≥n f√≠sica:** La primera y segunda fila representan la sensibilidad de las velocidades en marco del cuerpo respecto a velocidades globales y orientaci√≥n. Los t√©rminos $\frac{\partial v_{x,b}}{\partial \psi}$ capturan la variaci√≥n de la proyecci√≥n de velocidad al rotar el marco de referencia.

### 1.3 Manejo de √Ångulos

**Problema:** El √°ngulo $\psi \in [-\pi, \pi]$ presenta discontinuidades (e.g., $\pi$ y $-\pi$ representan la misma orientaci√≥n, pero su diferencia aritm√©tica es $2\pi$).

**Soluci√≥n implementada:**

En [`ekf_omnidirectional.py`](ekf_omnidirectional.py#L111):
```python
from state_estimation.common import make_residual_fn

ekf.set_residual_fn(make_residual_fn(angle_indices=[2]))  # psi en √≠ndice 2
```

La funci√≥n `make_residual_fn` ([`common/angles.py`](../state_estimation/common/angles.py)) envuelve la diferencia angular al intervalo $(-\pi, \pi]$:

$$
\text{residual}_{\psi}(a, b) = \text{atan2}(\sin(a - b), \cos(a - b))
$$

Esto garantiza que innovaciones angulares sean consistentes (e.g., $\psi_{\text{meas}} = -179¬∞$ y $\psi_{\text{pred}} = 179¬∞$ resultan en residual de $2¬∞$, no $358¬∞$).

### 1.4 Matrices de Covarianza

**Ruido de proceso** $\mathbf{Q}$ (calibrado experimentalmente):
$$
\mathbf{Q} = \text{diag}([1.23 \times 10^{-8}, 1.24 \times 10^{-8}, 1 \times 10^{-12}, 4.91 \times 10^{-4}, 4.97 \times 10^{-4}, 1 \times 10^{-12}])
$$

- Valores peque√±os en posici√≥n ($\sim 10^{-8}$): Alta confianza en modelo de integraci√≥n
- Valores mayores en velocidad ($\sim 10^{-4}$): Mayor incertidumbre en aceleraciones del IMU
- Valores m√≠nimos en $\psi$ y $\omega$: Confianza en modelo de orientaci√≥n

**Ruido de medici√≥n** $\mathbf{R}$:
$$
\mathbf{R} = \text{diag}([6.72 \times 10^{-4}, 6.72 \times 10^{-4}, 1.31 \times 10^{-2}, 4.06 \times 10^{-6}])
$$

Correspondiente a $[v_{x,b}, v_{y,b}, \omega, \psi]$. Los valores se derivan de caracterizaci√≥n experimental de encoders e IMU.

### 1.5 Configuraci√≥n Experimental

**Inicializaci√≥n** ([`ekf_omnidirectional.py`](ekf_omnidirectional.py#L95)):
```python
ekf.x = ground_truth[0]  # Estado inicial (sin error en experimentos controlados)
ekf.P = np.diag([0.5, 0.5, 0.1, 0.2, 0.2, 0.05])  # Covarianza inicial
```

**Frecuencia de actualizaci√≥n:** 100 Hz ($\Delta t = 0.01$ s)

**Flujo de ejecuci√≥n:**
```python
for k in range(N - 1):
    ekf.predict(u=controls[k], f=robot.dynamics, F=robot.jacobian_F)
    ekf.update(z=measurements[k + 1], h=robot.measurement, H=robot.jacobian_H)
    estimates[k + 1] = ekf.x
```

---

## 2. Unscented Kalman Filter (UKF)

**Archivo de implementaci√≥n:** [`filters/unscented.py`](../state_estimation/filters/unscented.py)  
**Script de ejecuci√≥n:** [`ukf_omnidirectional.py`](ukf_omnidirectional.py)

### 2.1 Fundamento Matem√°tico: Transformada Unscented

El UKF evita la linealizaci√≥n jacobiana del EKF mediante la **Transformada Unscented (UT)**, que propaga una distribuci√≥n Gaussiana a trav√©s de transformaciones no lineales usando un conjunto determin√≠stico de puntos sigma.

**Principio:** Es m√°s f√°cil aproximar una distribuci√≥n Gaussiana que aproximar funciones no lineales arbitrarias. La UT logra precisi√≥n de segundo orden (t√©rminos de Taylor hasta $\mathcal{O}(\Delta x^2)$) para cualquier no linealidad, mientras que el EKF solo es de primer orden.

#### Generaci√≥n de Sigma Points

Para un estado $\mathbf{x} \sim \mathcal{N}(\bar{\mathbf{x}}, \mathbf{P})$ de dimensi√≥n $n$, se generan $2n+1$ sigma points:

$$
\mathcal{X}^{(i)} = \begin{cases}
\bar{\mathbf{x}} & i = 0 \\
\bar{\mathbf{x}} + \left(\sqrt{(n + \lambda)\mathbf{P}}\right)_i & i = 1, \ldots, n \\
\bar{\mathbf{x}} - \left(\sqrt{(n + \lambda)\mathbf{P}}\right)_{i-n} & i = n+1, \ldots, 2n
\end{cases}
$$

Donde $\sqrt{\mathbf{P}}$ es la descomposici√≥n de Cholesky: $\mathbf{L}\mathbf{L}^T = \mathbf{P}$.

**Implementaci√≥n** ([`filters/unscented.py`](../state_estimation/filters/unscented.py#L54)):
```python
def sigma_points(self, x, P):
    U = cholesky((self.n + self._lambda) * P)  # scipy usa L^T
    sigmas = np.zeros((2*self.n + 1, self.n))
    sigmas[0] = x
    for k in range(self.n):
        sigmas[k + 1] = x + U[k]
        sigmas[n + k + 1] = x - U[k]
    return sigmas
```

**Nota:** `scipy.linalg.cholesky` retorna la matriz triangular superior $U$ donde $U^T U = A$. Se accede a las columnas como filas para construir las direcciones de los sigma points.

#### Par√°metros de Escalamiento: $\alpha$, $\beta$, $\kappa$

**Par√°metro de escala combinado:**
$$
\lambda = \alpha^2 (n + \kappa) - n
$$

**$\alpha \in (0, 1]$:** Controla la dispersi√≥n de sigma points alrededor de la media.
- Valores peque√±os ($\alpha \approx 0.001$): Sigma points muy cercanos a $\bar{\mathbf{x}}$ ‚Üí menor exploraci√≥n, menor influencia de no linealidades
- Valores grandes ($\alpha \approx 1$): Mayor dispersi√≥n ‚Üí captura mejor curvaturas de funciones no lineales
- **Valor usado:** $\alpha = 0.5$ (balance entre estabilidad y precisi√≥n)

**$\beta \geq 0$:** Incorpora conocimiento a priori de la distribuci√≥n.
- $\beta = 2$: √ìptimo para distribuciones Gaussianas (minimiza error de cuarto orden en curtosis)
- **Valor usado:** $\beta = 2.0$ (asumiendo Gaussianidad)

**$\kappa \in \mathbb{R}$:** Par√°metro de ajuste secundario.
- $\kappa = 3 - n$: Garantiza matriz semidefinida positiva
- $\kappa = 0$: Simplificaci√≥n com√∫n
- **Valor usado:** $\kappa = 0.0$

**Configuraci√≥n experimental** ([`ukf_omnidirectional.py`](ukf_omnidirectional.py#L107)):
```python
points = MerweScaledSigmaPoints(n=6, alpha=0.5, beta=2.0, kappa=0.0)
```

Con $n=6$, resulta en $\lambda = 0.5^2 \cdot 6 - 6 = -4.5$.

#### Pesos para Media y Covarianza

**Peso para la media:**
$$
W_m^{(i)} = \begin{cases}
\frac{\lambda}{n + \lambda} & i = 0 \\
\frac{1}{2(n + \lambda)} & i = 1, \ldots, 2n
\end{cases}
$$

**Peso para la covarianza:**
$$
W_c^{(0)} = \frac{\lambda}{n + \lambda} + (1 - \alpha^2 + \beta), \quad W_c^{(i)} = W_m^{(i)} \text{ para } i > 0
$$

El t√©rmino $(1 - \alpha^2 + \beta)$ en $W_c^{(0)}$ corrige efectos de orden superior en la covarianza.

**Implementaci√≥n** ([`filters/unscented.py`](../state_estimation/filters/unscented.py#L45)):
```python
self.Wm = np.full(2*n + 1, 0.5 / (n + self._lambda))
self.Wc = np.copy(self.Wm)
self.Wm[0] = self._lambda / (n + self._lambda)
self.Wc[0] = self._lambda / (n + self._lambda) + (1 - alpha**2 + beta)
```

Con los valores usados:
- $W_m^{(0)} = -4.5 / 1.5 = -3.0$
- $W_m^{(i)} = 0.5 / 1.5 = 0.333$ para $i > 0$
- $W_c^{(0)} = -3.0 + (1 - 0.25 + 2) = -0.25$

**Nota:** Pesos negativos son v√°lidos y surgen naturalmente del escalamiento de Merwe.

### 2.2 Predicci√≥n Unscented

**Algoritmo:**

1. **Generar sigma points** del estado a posteriori anterior:
   $$\mathcal{X}_{k-1|k-1}^{(i)} = \text{sigmaPoints}(\hat{\mathbf{x}}_{k-1|k-1}, \mathbf{P}_{k-1|k-1})$$

2. **Propagar** cada sigma point a trav√©s de la din√°mica:
   $$\mathcal{X}_{k|k-1}^{(i)} = f(\mathcal{X}_{k-1|k-1}^{(i)}, \mathbf{u}_k)$$

3. **Calcular media a priori**:
   $$\hat{\mathbf{x}}_{k|k-1} = \sum_{i=0}^{2n} W_m^{(i)} \mathcal{X}_{k|k-1}^{(i)}$$

4. **Calcular covarianza a priori**:
   $$\mathbf{P}_{k|k-1} = \sum_{i=0}^{2n} W_c^{(i)} [\mathcal{X}_{k|k-1}^{(i)} - \hat{\mathbf{x}}_{k|k-1}][\mathcal{X}_{k|k-1}^{(i)} - \hat{\mathbf{x}}_{k|k-1}]^T + \mathbf{Q}_k$$

**Implementaci√≥n** ([`filters/unscented.py`](../state_estimation/filters/unscented.py#L180)):
```python
def predict(self, u=None, f=None, Q=None):
    sigmas = self.points.sigma_points(self.x, self.P)
    for i, s in enumerate(sigmas):
        self.sigmas_f[i] = f(s, u)
    self.x = np.dot(self.points.Wm, self.sigmas_f)
    self.P = self._unscented_transform_cov(self.sigmas_f, self.x, 
                                            self.points.Wc, Q, self._residual_x_fn)
```

### 2.3 Actualizaci√≥n Unscented

**Algoritmo:**

1. **Propagar** sigma points predichos a trav√©s de la medici√≥n:
   $$\mathcal{Z}_{k|k-1}^{(i)} = h(\mathcal{X}_{k|k-1}^{(i)})$$

2. **Medici√≥n predicha**:
   $$\hat{\mathbf{z}}_{k|k-1} = \sum_{i=0}^{2n} W_m^{(i)} \mathcal{Z}_{k|k-1}^{(i)}$$

3. **Covarianza de innovaci√≥n**:
   $$\mathbf{S}_k = \sum_{i=0}^{2n} W_c^{(i)} [\mathcal{Z}_{k|k-1}^{(i)} - \hat{\mathbf{z}}_{k|k-1}][\mathcal{Z}_{k|k-1}^{(i)} - \hat{\mathbf{z}}_{k|k-1}]^T + \mathbf{R}_k$$

4. **Covarianza cruzada** estado-medici√≥n:
   $$\mathbf{P}_{xz} = \sum_{i=0}^{2n} W_c^{(i)} [\mathcal{X}_{k|k-1}^{(i)} - \hat{\mathbf{x}}_{k|k-1}][\mathcal{Z}_{k|k-1}^{(i)} - \hat{\mathbf{z}}_{k|k-1}]^T$$

5. **Ganancia de Kalman**:
   $$\mathbf{K}_k = \mathbf{P}_{xz} \mathbf{S}_k^{-1}$$

6. **Actualizaci√≥n del estado**:
   $$\hat{\mathbf{x}}_{k|k} = \hat{\mathbf{x}}_{k|k-1} + \mathbf{K}_k (\mathbf{z}_k - \hat{\mathbf{z}}_{k|k-1})$$

7. **Actualizaci√≥n de covarianza**:
   $$\mathbf{P}_{k|k} = \mathbf{P}_{k|k-1} - \mathbf{K}_k \mathbf{S}_k \mathbf{K}_k^T$$

**Implementaci√≥n** ([`filters/unscented.py`](../state_estimation/filters/unscented.py#L220)):
```python
def update(self, z, h=None, R=None):
    for i, s in enumerate(self.sigmas_f):
        self.sigmas_h[i] = h(s)
    z_mean = np.dot(self.points.Wm, self.sigmas_h)
    
    S = self._unscented_transform_cov(self.sigmas_h, z_mean, 
                                       self.points.Wc, R, self._residual_z_fn)
    Pxz = self._cross_covariance(self.sigmas_f, self.x, 
                                   self.sigmas_h, z_mean, self.points.Wc)
    K = Pxz @ np.linalg.inv(S)
    y = self._residual_z_fn(z, z_mean)
    self.x = self.x + K @ y
    self.P = self.P - K @ S @ K.T
```

### 2.4 Funciones Personalizadas para √Ångulos

#### Funci√≥n de Media Circular

Para estados con componentes angulares, la media aritm√©tica es inadecuada. Se usa **media circular**:

$$
\bar{\psi} = \text{atan2}\left(\sum_{i=0}^{2n} W_m^{(i)} \sin\psi^{(i)}, \sum_{i=0}^{2n} W_m^{(i)} \cos\psi^{(i)}\right)
$$

**Implementaci√≥n** ([`ukf_omnidirectional.py`](ukf_omnidirectional.py#L47)):
```python
from state_estimation.common.angles import circular_mean

def state_mean_fn(sigmas, Wm):
    x = np.zeros(6)
    x[0] = np.dot(Wm, sigmas[:, 0])  # x (lineal)
    x[1] = np.dot(Wm, sigmas[:, 1])  # y (lineal)
    x[2] = circular_mean(sigmas[:, 2], Wm)  # psi (circular)
    x[3] = np.dot(Wm, sigmas[:, 3])  # vx (lineal)
    x[4] = np.dot(Wm, sigmas[:, 4])  # vy (lineal)
    x[5] = np.dot(Wm, sigmas[:, 5])  # omega (lineal)
    return x
```

La funci√≥n `circular_mean` ([`common/angles.py`](../state_estimation/common/angles.py)) proyecta √°ngulos al c√≠rculo unitario, promedia vectores 2D y recupera el √°ngulo:

```python
def circular_mean(angles, weights):
    sin_sum = np.sum(weights * np.sin(angles))
    cos_sum = np.sum(weights * np.cos(angles))
    return np.arctan2(sin_sum, cos_sum)
```

**Justificaci√≥n:** La media aritm√©tica de $\psi = [179¬∞, -179¬∞]$ es $0¬∞$ (incorrecto, deber√≠a ser $\pm 180¬∞$). La media circular correctamente resulta en $180¬∞$.

#### Funci√≥n de Residual Angular

Para diferencias en covarianza, los √°ngulos deben normalizarse:

$$
\text{residual}(\psi_a, \psi_b) = \text{atan2}(\sin(\psi_a - \psi_b), \cos(\psi_a - \psi_b))
$$

**Implementaci√≥n** ([`ukf_omnidirectional.py`](ukf_omnidirectional.py#L71)):
```python
from state_estimation.common.angles import normalize_angle

def residual_x(a, b):
    y = a - b
    y[2] = normalize_angle(y[2])  # psi
    return y

def residual_z(a, b):
    y = a - b
    y[3] = normalize_angle(y[3])  # psi en medici√≥n
    return y
```

**Configuraci√≥n del UKF** ([`ukf_omnidirectional.py`](ukf_omnidirectional.py#L131)):
```python
ukf.set_mean_fn(x_mean_fn=state_mean_fn, z_mean_fn=z_mean_fn)
ukf.set_residual_fn(residual_x_fn=residual_x, residual_z_fn=residual_z)
```

### 2.5 Matrices de Covarianza

**Ruido de proceso:**
$$
\mathbf{Q} = \text{diag}([1 \times 10^{-4}, 1 \times 10^{-4}, 1 \times 10^{-5}, 5 \times 10^{-3}, 5 \times 10^{-3}, 5 \times 10^{-4}]) \times 10
$$

**Ruido de medici√≥n:**
$$
\mathbf{R} = \text{diag}([6.72 \times 10^{-4}, 6.72 \times 10^{-4}, 1.31 \times 10^{-2}, 1.22 \times 10^{-3}]) \times 10^3
$$

Escalados para mejorar condicionamiento num√©rico del UKF.

---

## 3. Particle Filter (PF)

**Archivo de implementaci√≥n:** [`filters/particle.py`](../state_estimation/filters/particle.py)  
**Script de ejecuci√≥n:** [`pf_omnidirectional.py`](pf_omnidirectional.py)

### 3.1 Fundamento: Filtrado Secuencial de Importancia

El Particle Filter representa la distribuci√≥n posterior $p(\mathbf{x}_k | \mathbf{z}_{1:k}, \mathbf{u}_{1:k})$ mediante un conjunto de **part√≠culas** (muestras Monte Carlo):

$$
p(\mathbf{x}_k | \mathbf{z}_{1:k}) \approx \sum_{i=1}^{N} w_k^{(i)} \delta(\mathbf{x}_k - \mathbf{x}_k^{(i)})
$$

Donde:
- $\mathbf{x}_k^{(i)}$: Estado de la part√≠cula $i$
- $w_k^{(i)}$: Peso normalizado ($\sum_i w_k^{(i)} = 1$)
- $N$: N√∫mero de part√≠culas (t√≠picamente 1000-10000)

**Ventajas:**
- Maneja no linealidades arbitrarias y distribuciones multimodales
- No asume Gaussianidad
- Converge a la distribuci√≥n verdadera cuando $N \to \infty$

**Desventajas:**
- Complejidad computacional $\mathcal{O}(N)$
- Degeneraci√≥n de part√≠culas requiere remuestreo

### 3.2 Algoritmo SIR (Sequential Importance Resampling)

Implementado en [`filters/particle.py`](../state_estimation/filters/particle.py).

#### Paso 1: Inicializaci√≥n

Muestrear part√≠culas de la distribuci√≥n inicial:

$$
\mathbf{x}_0^{(i)} \sim \mathcal{N}(\mathbf{x}_0, \mathbf{P}_0), \quad w_0^{(i)} = \frac{1}{N}
$$

**Implementaci√≥n** ([`pf_omnidirectional.py`](pf_omnidirectional.py#L70)):
```python
x0 = ground_truth[0]
P0 = np.diag([0.5, 0.5, 0.1, 0.2, 0.2, 0.05]) * 1e-5
pf.initialize_particles(x0=x0, P0=P0)
```

Genera 2000 part√≠culas con dispersi√≥n inicial muy baja (alta confianza en estado inicial).

#### Paso 2: Predicci√≥n (Propagaci√≥n)

Cada part√≠cula se propaga a trav√©s de la din√°mica con ruido de proceso:

$$
\mathbf{x}_k^{(i)} = f(\mathbf{x}_{k-1}^{(i)}, \mathbf{u}_k) + \mathbf{w}_k^{(i)}, \quad \mathbf{w}_k^{(i)} \sim \mathcal{N}(\mathbf{0}, \mathbf{Q})
$$

**Implementaci√≥n** ([`filters/particle.py`](../state_estimation/filters/particle.py#L106)):
```python
def predict(self, u=None, f=None, process_noise_std=None):
    for i in range(self.N):
        self.particles[i] = f(self.particles[i], u)
    
    # A√±adir ruido Gaussiano
    if process_noise_std is not None:
        noise = np.random.normal(0, process_noise_std, size=self.particles.shape)
        self.particles += noise
    
    self.x = np.average(self.particles, weights=self.weights, axis=0)
```

**Configuraci√≥n experimental** ([`pf_omnidirectional.py`](pf_omnidirectional.py#L76)):
```python
process_std = np.array([0.01, 0.01, 0.01, 0.05, 0.05, 0.05])
```

Ruido mayor en velocidades que en posiciones/orientaci√≥n.

#### Paso 3: Actualizaci√≥n (Repesado)

Los pesos se actualizan seg√∫n la verosimilitud de la medici√≥n:

$$
w_k^{(i)} \propto w_{k-1}^{(i)} \cdot p(\mathbf{z}_k | \mathbf{x}_k^{(i)})
$$

Para mediciones Gaussianas:

$$
p(\mathbf{z}_k | \mathbf{x}_k^{(i)}) = \frac{1}{(2\pi)^{m/2}|\mathbf{R}|^{1/2}} \exp\left(-\frac{1}{2}[\mathbf{z}_k - h(\mathbf{x}_k^{(i)})]^T \mathbf{R}^{-1} [\mathbf{z}_k - h(\mathbf{x}_k^{(i)})]\right)
$$

**Implementaci√≥n** ([`filters/particle.py`](../state_estimation/filters/particle.py#L148)):
```python
def update(self, z, h=None, R=None):
    likelihood_fn = self._make_gaussian_likelihood(h, R)
    
    for i in range(self.N):
        self.weights[i] *= likelihood_fn(z, self.particles[i])
    
    # Normalizar pesos
    self.weights /= np.sum(self.weights)
    
    self.x = np.average(self.particles, weights=self.weights, axis=0)
    self.P = self._compute_covariance()
```

La funci√≥n `_make_gaussian_likelihood` ([`filters/particle.py`](../state_estimation/filters/particle.py#L364)) implementa la verosimilitud Gaussiana multivariada usando `scipy.stats.multivariate_normal`.

#### Paso 4: Remuestreo Sistem√°tico

**Problema de degeneraci√≥n:** Tras varias iteraciones, la mayor√≠a de part√≠culas tienen pesos despreciables ($w^{(i)} \approx 0$), concentr√°ndose la masa probabil√≠stica en pocas part√≠culas.

**M√©trica:** Tama√±o efectivo de muestra (ESS):

$$
N_{\text{eff}} = \frac{1}{\sum_{i=1}^{N} (w^{(i)})^2}
$$

- $N_{\text{eff}} = N$: Pesos uniformes (ideal)
- $N_{\text{eff}} = 1$: Un peso domina (degeneraci√≥n total)

**Criterio de remuestreo:** Si $N_{\text{eff}} < N / 2$, aplicar remuestreo.

**Algoritmo de remuestreo sistem√°tico** ([`filters/particle.py`](../state_estimation/filters/particle.py#L234)):

1. Generar $N$ posiciones equiespaciadas con offset aleatorio:
   $$u_i = \frac{i + \mathcal{U}(0,1)}{N}, \quad i = 0, \ldots, N-1$$

2. Calcular CDF de pesos: $c_j = \sum_{k=0}^{j} w^{(k)}$

3. Para cada $u_i$, encontrar $j$ tal que $c_{j-1} < u_i \leq c_j$ y duplicar part√≠cula $j$

**Ventajas sobre multinomial:**
- Menor varianza (muestreo estratificado)
- Complejidad $\mathcal{O}(N)$ en lugar de $\mathcal{O}(N \log N)$

**Implementaci√≥n:**
```python
def _systematic_resample(self):
    positions = (np.arange(self.N) + np.random.random()) / self.N
    indices = np.zeros(self.N, dtype=int)
    cumsum = np.cumsum(self.weights)
    
    i, j = 0, 0
    while i < self.N:
        if positions[i] < cumsum[j]:
            indices[i] = j
            i += 1
        else:
            j += 1
    return indices
```

**Uso en el filtro** ([`pf_omnidirectional.py`](pf_omnidirectional.py#L87)):
```python
for k in range(N - 1):
    pf.predict(u=controls[k], f=robot.dynamics, process_noise_std=process_std)
    pf.update(z=measurements[k + 1], h=robot.measurement, R=R)
    pf.resample(scheme='systematic')  # Remuestreo en cada paso
```

**Nota:** En la implementaci√≥n actual, se aplica remuestreo en cada iteraci√≥n. Alternativamente, se puede condicionar al ESS:

```python
if pf.effective_sample_size() < pf.resample_threshold:
    pf.resample(scheme='systematic')
```

### 3.3 Estimaci√≥n de Estado

**Media ponderada:**
$$
\hat{\mathbf{x}}_k = \sum_{i=1}^{N} w_k^{(i)} \mathbf{x}_k^{(i)}
$$

**Covarianza:**
$$
\mathbf{P}_k = \sum_{i=1}^{N} w_k^{(i)} (\mathbf{x}_k^{(i)} - \hat{\mathbf{x}}_k)(\mathbf{x}_k^{(i)} - \hat{\mathbf{x}}_k)^T
$$

**Implementaci√≥n** ([`filters/particle.py`](../state_estimation/filters/particle.py#L343)):
```python
def _compute_covariance(self):
    mean = self.x
    diff = self.particles - mean
    P = np.zeros((self.dim_x, self.dim_x))
    for i in range(self.N):
        d = diff[i].reshape(-1, 1)
        P += self.weights[i] * (d @ d.T)
    return P
```

### 3.4 Configuraci√≥n Experimental

**N√∫mero de part√≠culas:** $N = 2000$ (balance entre precisi√≥n y costo computacional)

**Matrices de ruido:**
```python
process_std = np.array([0.01, 0.01, 0.01, 0.05, 0.05, 0.05])
R = np.diag([6.72e-4, 6.72e-4, 1.3125e-2, 1.218e-3])
```

**Frecuencia:** 100 Hz (igual que EKF/UKF)

**M√©tricas de monitoreo:**
```python
if (k + 1) % 100 == 0:
    ess = pf.effective_sample_size()
    print(f"  Step {k+1}/{N-1}, ESS: {ess:.1f}")
```

Valores t√≠picos de ESS: 800-1500 (de 2000), indicando distribuci√≥n saludable de pesos.

---

## 4. Comparaci√≥n Metodol√≥gica

### Tabla Comparativa

| Caracter√≠stica | EKF | UKF | PF |
|----------------|-----|-----|-----|
| **Linealizaci√≥n** | Jacobiana (1er orden) | Transformada Unscented (2do orden) | No requerida |
| **Distribuci√≥n** | Gaussiana | Gaussiana | Arbitraria |
| **Complejidad** | $\mathcal{O}(n^3)$ | $\mathcal{O}(n^3)$ | $\mathcal{O}(N \cdot n)$ |
| **Jacobiano** | ‚úÖ Requerido | ‚ùå No requerido | ‚ùå No requerido |
| **Precisi√≥n** | Baja (no lineal fuerte) | Alta (no lineal moderada) | Converge a √≥ptimo |
| **Estabilidad num√©rica** | Sensible | Robusta | Muy robusta |
| **Multimodalidad** | ‚ùå | ‚ùå | ‚úÖ |
| **Tiempo ejecuci√≥n** | ~1 ms/iter | ~2 ms/iter | ~20 ms/iter |

### Escenarios de Aplicaci√≥n

**EKF:**
- Din√°micas d√©bilmente no lineales
- Recursos computacionales limitados
- Modelo anal√≠tico bien caracterizado

**UKF:**
- No linealidades moderadas
- Jacobiano dif√≠cil de derivar o costoso
- Mayor precisi√≥n sin incremento dram√°tico de costo

**PF:**
- No linealidades severas (e.g., localizaci√≥n con ambig√ºedad)
- Distribuciones no Gaussianas (e.g., fallas intermitentes)
- Recursos computacionales abundantes

Para el robot omnidireccional con din√°mica de cuerpo r√≠gido, **EKF y UKF** suelen ser suficientes, siendo PF un caso de validaci√≥n para distribuciones potencialmente multimodales en orientaci√≥n.

---

## 5. Fuentes de Datos Experimentales

### 5.1 Estructura de Archivos

```
data/
‚îú‚îÄ‚îÄ sensors/              # Datos crudos de IMU + encoders
‚îÇ   ‚îî‚îÄ‚îÄ expN.txt         # N = 1-10
‚îî‚îÄ‚îÄ processed/
    ‚îî‚îÄ‚îÄ trajectories/    # Trayectorias fusionadas (video + IMU)
        ‚îî‚îÄ‚îÄ traj_vid_N.csv
```

### 5.2 Formato de Datos de Sensores

**Archivo:** `data/sensors/expN.txt`

```
t,ax,ay,alpha,w1,w2,w3,u1,u2,u3,vbx_sp,vby_sp,wb_sp
```

- `t`: Tiempo [s]
- `ax, ay`: Aceleraci√≥n del IMU en marco del cuerpo [m/s¬≤]
- `alpha`: Orientaci√≥n del IMU [rad]
- `w1, w2, w3`: Velocidades angulares de ruedas [rad/s]
- `u1, u2, u3`: Se√±ales PWM a motores [%]
- `*_sp`: Setpoints (no usados en estimaci√≥n)

### 5.3 Formato de Trayectorias Ground Truth

**Archivo:** `data/processed/trajectories/traj_vid_N.csv`

```
time_s,x_m,y_m,phi_rad,vx_m_s,vy_m_s,omega_rad_s,u1_pwm,u2_pwm,u3_pwm
```

- Obtenido mediante flujo √≥ptico + detecci√≥n de color (ver [Robot_Identification/src/examples/README.md](../../Robot_Identification/src/examples/README.md))
- Frecuencia: 100 Hz (resampleado)
- Usado como referencia para m√©tricas de error

### 5.4 Carga de Datos

Implementado en [`trajectory_generators.py`](trajectory_generators.py#L150):

```python
def load_experimental_data(sensors_path, trajectory_path, N=1000, dt=0.01):
    # Cargar sensores
    sensors_df = pd.read_csv(sensors_path)
    ax = sensors_df['ax'].values[:N]
    ay = sensors_df['ay'].values[:N]
    
    # Cargar ground truth
    traj_df = pd.read_csv(trajectory_path)
    x = traj_df['x_m'].values[:N]
    y = traj_df['y_m'].values[:N]
    phi = traj_df['phi_rad'].values[:N]
    
    # Construir mediciones: [vx_b, vy_b, omega, psi]
    measurements = compute_body_velocities(vx, vy, phi, omega)
    
    # Controles: [ax_b, ay_b]
    controls = np.column_stack([ax, ay])
    
    return {'time': t, 'controls': controls, 'measurements': measurements,
            'ground_truth': ground_truth, 'dt': dt}
```

---

## 6. M√©tricas de Evaluaci√≥n

### 6.1 Errores de Estimaci√≥n

**Root Mean Square Error (RMSE):**
$$
\text{RMSE}_i = \sqrt{\frac{1}{N} \sum_{k=1}^{N} (x_k^{(i)} - \hat{x}_k^{(i)})^2}
$$

Para cada componente del estado $i \in \{x, y, \psi, v_x, v_y, \omega\}$.

**Mean Absolute Error (MAE):**
$$
\text{MAE}_i = \frac{1}{N} \sum_{k=1}^{N} |x_k^{(i)} - \hat{x}_k^{(i)}|
$$

### 6.2 Consistencia del Filtro

**Normalized Estimation Error Squared (NEES):**
$$
\epsilon_k = (\mathbf{x}_k - \hat{\mathbf{x}}_k)^T \mathbf{P}_k^{-1} (\mathbf{x}_k - \hat{\mathbf{x}}_k)
$$

Para un filtro consistente, $\epsilon_k \sim \chi^2(n)$ con $n$ grados de libertad. Valor esperado: $\mathbb{E}[\epsilon_k] = n = 6$.

**Normalized Innovation Squared (NIS):**
$$
\nu_k = \mathbf{y}_k^T \mathbf{S}_k^{-1} \mathbf{y}_k
$$

Donde $\mathbf{y}_k$ es la innovaci√≥n. Para filtro consistente, $\nu_k \sim \chi^2(m)$ con $m = 4$ (dimensi√≥n de medici√≥n).

**Implementaci√≥n:** [`metrics.py`](../state_estimation/metrics.py)

### 6.3 Resultados T√≠picos

**EKF:**
- RMSE posici√≥n: 0.03-0.05 m
- RMSE orientaci√≥n: 0.05-0.10 rad
- NEES promedio: 5-8 (ligeramente optimista)

**UKF:**
- RMSE posici√≥n: 0.025-0.04 m
- RMSE orientaci√≥n: 0.04-0.08 rad
- NEES promedio: 6-9 (m√°s consistente)

**PF (N=2000):**
- RMSE posici√≥n: 0.02-0.035 m
- RMSE orientaci√≥n: 0.03-0.06 rad
- ESS promedio: 900-1400

---

## 7. Ejecuci√≥n de Experimentos

### 7.1 Scripts Principales

**EKF:**
```bash
python ekf_omnidirectional.py
```

**UKF:**
```bash
python ukf_omnidirectional.py
```

**PF:**
```bash
python pf_omnidirectional.py
```

### 7.2 Configuraci√≥n

Editar constantes al inicio de cada script:

```python
USE_EXPERIMENTAL_DATA = True   # True: datos reales, False: sint√©ticos
EXPERIMENT_NUMBER = 2          # Experimento 1-10
N_POINTS = 1000                # N√∫mero de pasos temporales
DT = 0.01                      # Periodo de muestreo (100 Hz)
```

### 7.3 Salidas

**Directorio de resultados:**
```
results/estimation/
‚îú‚îÄ‚îÄ ekf/
‚îÇ   ‚îú‚îÄ‚îÄ ekf_trayectoria_2d.png
‚îÇ   ‚îú‚îÄ‚îÄ ekf_estados_temporales.png
‚îÇ   ‚îî‚îÄ‚îÄ ekf_metricas.csv
‚îú‚îÄ‚îÄ ukf/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ pf/
    ‚îî‚îÄ‚îÄ ...
```

**Formato CSV de m√©tricas:**
```csv
M√©trica,Valor
RMSE Total,0.0347
RMSE X [m],0.0234
RMSE Y [m],0.0221
RMSE Phi [rad],0.0587
NEES Promedio,6.823
```

---

## 8. Consideraciones de Implementaci√≥n

### 8.1 Estabilidad Num√©rica

**Problema:** Matrices de covarianza pueden perder simetr√≠a o positividad definida por errores de punto flotante.

**Soluciones implementadas:**

1. **Forma de Joseph para EKF** ([`extended.py`](../state_estimation/filters/extended.py#L195)):
   $$\mathbf{P} = (\mathbf{I} - \mathbf{K}\mathbf{H})\mathbf{P}(\mathbf{I} - \mathbf{K}\mathbf{H})^T + \mathbf{K}\mathbf{R}\mathbf{K}^T$$
   
   Garantiza simetr√≠a y mejora condicionamiento num√©rico.

2. **Descomposici√≥n de Cholesky con manejo de errores** ([`unscented.py`](../state_estimation/filters/unscented.py#L76)):
   ```python
   try:
       U = cholesky((self.n + self._lambda) * P)
   except np.linalg.LinAlgError:
       # Si falla, forzar simetr√≠a y a√±adir regularizaci√≥n
       P = (P + P.T) / 2
       P += np.eye(self.n) * 1e-9
       U = cholesky((self.n + self._lambda) * P)
   ```

3. **Normalizaci√≥n de pesos en PF** ([`particle.py`](../state_estimation/filters/particle.py#L183)):
   ```python
   weight_sum = np.sum(self.weights)
   if weight_sum < 1e-10:  # Degeneraci√≥n extrema
       self.weights = np.ones(self.N) / self.N  # Reiniciar uniformemente
   else:
       self.weights /= weight_sum
   ```

### 8.2 Eficiencia Computacional

**Vectorizaci√≥n de operaciones:**

- EKF/UKF: Operaciones matriciales con NumPy/SciPy (BLAS optimizado)
- PF: Bucles sobre part√≠culas inevitables, pero operaciones internas vectorizadas

**Perfilado t√≠pico (Intel i7, 2.6 GHz):**
- EKF: ~0.8 ms/iteraci√≥n
- UKF: ~1.5 ms/iteraci√≥n
- PF (N=2000): ~18 ms/iteraci√≥n

**Paralelizaci√≥n potencial:** El PF es embarazosamente paralelizable (cada part√≠cula se procesa independientemente). Se puede usar `multiprocessing` o Numba JIT para acelerar.

### 8.3 Afinaci√≥n de Par√°metros

**Proceso de calibraci√≥n de $\mathbf{Q}$ y $\mathbf{R}$:**

1. **Caracterizaci√≥n de sensores:**
   - Colocar robot est√°tico, medir $\sigma_{\text{sensor}}$ durante 1 minuto
   - Diagonales de $\mathbf{R}$: varianzas emp√≠ricas

2. **Optimizaci√≥n de $\mathbf{Q}$:**
   - Iniciar con valores heur√≠sticos: $\mathbf{Q} = 0.01 \cdot \mathbb{E}[\mathbf{x}\mathbf{x}^T]$
   - Ejecutar filtro, calcular NEES
   - Ajustar iterativamente buscando NEES $\approx n$
   - Herramientas: Grid search o CMA-ES

3. **Validaci√≥n cruzada:**
   - Calibrar en experimentos 1-5
   - Validar en experimentos 6-10
   - Reportar m√©tricas de ambos conjuntos

---

## Referencias Metodol√≥gicas

### Libros Fundamentales

1. **Thrun, S., Burgard, W., & Fox, D. (2005).** *Probabilistic Robotics*. MIT Press.  
   Cap√≠tulos 3 (Recursive State Estimation), 7 (Kalman Filters), 8 (EKF), 9 (UKF), 4 (Particle Filters).

2. **Bar-Shalom, Y., Li, X. R., & Kirubarajan, T. (2001).** *Estimation with Applications to Tracking and Navigation*. Wiley.  
   Secciones sobre Jacobiano del EKF y validaci√≥n NEES/NIS.

3. **S√§rkk√§, S. (2013).** *Bayesian Filtering and Smoothing*. Cambridge University Press.  
   Derivaciones rigurosas de UKF y transformada Unscented.

### Art√≠culos Espec√≠ficos

4. **Julier, S. J., & Uhlmann, J. K. (1997).** "New Extension of the Kalman Filter to Nonlinear Systems". *Signal Processing, Sensor Fusion, and Target Recognition VI*, SPIE.  
   Introducci√≥n original del UKF.

5. **Van Der Merwe, R., & Wan, E. A. (2001).** "The Square-Root Unscented Kalman Filter for State and Parameter-Estimation". *IEEE International Conference on Acoustics, Speech, and Signal Processing*.  
   Algoritmo de sigma points escalados (usado en esta implementaci√≥n).

6. **Arulampalam, M. S., et al. (2002).** "A Tutorial on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking". *IEEE Transactions on Signal Processing*, 50(2), 174-188.  
   Tutorial exhaustivo de PF con an√°lisis de m√©todos de remuestreo.

### Software y Bibliotecas

7. **FilterPy (Roger R. Labbe Jr.):** Biblioteca Python de referencia para filtros bayesianos.  
   Disponible en: https://github.com/rlabbe/filterpy  
   (Inspiraci√≥n para la API de esta implementaci√≥n)

---

## Ap√©ndices

### A. Nomenclatura y Convenciones

| S√≠mbolo | Descripci√≥n | Dimensi√≥n |
|---------|-------------|-----------|
| $\mathbf{x}_k$ | Vector de estado en tiempo $k$ | $6 \times 1$ |
| $\mathbf{u}_k$ | Vector de control | $2 \times 1$ |
| $\mathbf{z}_k$ | Vector de medici√≥n | $4 \times 1$ |
| $\mathbf{P}_k$ | Covarianza de estado | $6 \times 6$ |
| $\mathbf{Q}$ | Covarianza de ruido de proceso | $6 \times 6$ |
| $\mathbf{R}$ | Covarianza de ruido de medici√≥n | $4 \times 4$ |
| $\mathbf{F}_k$ | Jacobiano de din√°mica | $6 \times 6$ |
| $\mathbf{H}_k$ | Jacobiano de medici√≥n | $4 \times 6$ |
| $\mathbf{K}_k$ | Ganancia de Kalman | $6 \times 4$ |
| $n$ | Dimensi√≥n del estado | 6 |
| $m$ | Dimensi√≥n de la medici√≥n | 4 |
| $N$ | N√∫mero de part√≠culas (PF) | 1000-10000 |
| $\Delta t$ | Periodo de muestreo | 0.01 s |

### B. Pseudoc√≥digo Completo

#### EKF
```
INITIALIZE: x‚ÇÄ, P‚ÇÄ, Q, R

FOR k = 1 TO N:
    # Predict
    xÃÑ‚Çñ = f(x‚Çñ‚Çã‚ÇÅ, u‚Çñ)
    F‚Çñ = ‚àÇf/‚àÇx|(x‚Çñ‚Çã‚ÇÅ, u‚Çñ)
    PÃÑ‚Çñ = F‚Çñ P‚Çñ‚Çã‚ÇÅ F‚Çñ·µÄ + Q
    
    # Update
    y‚Çñ = z‚Çñ - h(xÃÑ‚Çñ)
    H‚Çñ = ‚àÇh/‚àÇx|xÃÑ‚Çñ
    S‚Çñ = H‚Çñ PÃÑ‚Çñ H‚Çñ·µÄ + R
    K‚Çñ = PÃÑ‚Çñ H‚Çñ·µÄ S‚Çñ‚Åª¬π
    x‚Çñ = xÃÑ‚Çñ + K‚Çñ y‚Çñ
    P‚Çñ = (I - K‚Çñ H‚Çñ) PÃÑ‚Çñ (I - K‚Çñ H‚Çñ)·µÄ + K‚Çñ R K‚Çñ·µÄ
END FOR
```

#### UKF
```
INITIALIZE: x‚ÇÄ, P‚ÇÄ, Q, R, Œ±, Œ≤, Œ∫

FOR k = 1 TO N:
    # Generate sigma points
    ùí≥‚Çñ‚Çã‚ÇÅ = sigmaPoints(x‚Çñ‚Çã‚ÇÅ, P‚Çñ‚Çã‚ÇÅ)
    
    # Predict
    FOR i = 0 TO 2n:
        ùí≥‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ = f(ùí≥‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ, u‚Çñ)
    END FOR
    xÃÑ‚Çñ = Œ£·µ¢ W‚Çò‚ÅΩ‚Å±‚Åæ ùí≥‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ
    PÃÑ‚Çñ = Œ£·µ¢ Wc‚ÅΩ‚Å±‚Åæ (ùí≥‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ - xÃÑ‚Çñ)(ùí≥‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ - xÃÑ‚Çñ)·µÄ + Q
    
    # Update
    FOR i = 0 TO 2n:
        ùíµ‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ = h(ùí≥‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ)
    END FOR
    zÃÑ‚Çñ = Œ£·µ¢ W‚Çò‚ÅΩ‚Å±‚Åæ ùíµ‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ
    S‚Çñ = Œ£·µ¢ Wc‚ÅΩ‚Å±‚Åæ (ùíµ‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ - zÃÑ‚Çñ)(ùíµ‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ - zÃÑ‚Çñ)·µÄ + R
    P‚Çì·µß = Œ£·µ¢ Wc‚ÅΩ‚Å±‚Åæ (ùí≥‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ - xÃÑ‚Çñ)(ùíµ‚Çñ|‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ - zÃÑ‚Çñ)·µÄ
    K‚Çñ = P‚Çì·µß S‚Çñ‚Åª¬π
    x‚Çñ = xÃÑ‚Çñ + K‚Çñ (z‚Çñ - zÃÑ‚Çñ)
    P‚Çñ = PÃÑ‚Çñ - K‚Çñ S‚Çñ K‚Çñ·µÄ
END FOR
```

#### PF
```
INITIALIZE: {x‚Çñ‚ÅΩ‚Å±‚Åæ, w‚Çñ‚ÅΩ‚Å±‚Åæ}·µ¢‚Çå‚ÇÅ·¥∫ ~ N(x‚ÇÄ, P‚ÇÄ), w‚Çñ‚ÅΩ‚Å±‚Åæ = 1/N

FOR k = 1 TO N:
    # Predict
    FOR i = 1 TO N:
        x‚Çñ‚ÅΩ‚Å±‚Åæ = f(x‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ, u‚Çñ) + ùí©(0, Q)
    END FOR
    
    # Update
    FOR i = 1 TO N:
        w‚Çñ‚ÅΩ‚Å±‚Åæ = w‚Çñ‚Çã‚ÇÅ‚ÅΩ‚Å±‚Åæ ¬∑ p(z‚Çñ | x‚Çñ‚ÅΩ‚Å±‚Åæ)
    END FOR
    w‚Çñ = w‚Çñ / Œ£·µ¢ w‚Çñ‚ÅΩ‚Å±‚Åæ
    
    # Resample
    IF ESS < N/2:
        {x‚Çñ‚ÅΩ‚Å±‚Åæ}·µ¢‚Çå‚ÇÅ·¥∫ = systematicResample({x‚Çñ‚ÅΩ‚Å±‚Åæ, w‚Çñ‚ÅΩ‚Å±‚Åæ})
        w‚Çñ‚ÅΩ‚Å±‚Åæ = 1/N ‚àÄi
    END IF
    
    # Estimate
    x‚Çñ = Œ£·µ¢ w‚Çñ‚ÅΩ‚Å±‚Åæ x‚Çñ‚ÅΩ‚Å±‚Åæ
END FOR
```

### C. C√≥digo de Ejemplo M√≠nimo

**Filtro EKF completo en ~20 l√≠neas:**

```python
from state_estimation import ExtendedKalmanFilter
from state_estimation.models import OmnidirectionalRobot
from state_estimation.common import make_residual_fn

# Inicializar
robot = OmnidirectionalRobot(dt=0.01)
ekf = ExtendedKalmanFilter(dim_x=6, dim_z=4, dim_u=2)
ekf.x = np.array([0, 0, 0, 0, 0, 0])
ekf.P = np.diag([0.5, 0.5, 0.1, 0.2, 0.2, 0.05])
ekf.Q = np.diag([1e-4, 1e-4, 1e-5, 5e-3, 5e-3, 5e-4])
ekf.R = np.diag([6.72e-4, 6.72e-4, 1.3125e-2, 1.218e-3])
ekf.set_residual_fn(make_residual_fn(angle_indices=[2]))

# Loop principal
for k in range(N - 1):
    ekf.predict(u=controls[k], f=robot.dynamics, F=robot.jacobian_F)
    ekf.update(z=measurements[k + 1], h=robot.measurement, H=robot.jacobian_H)
    estimates[k + 1] = ekf.x
```

---

**Autor:** Maverick Sossa Tob√≥n 
**Fecha:** Enero 2026  
**Versi√≥n:** 1.0  
**Instituci√≥n:** [Universidad de Antioquia]  
